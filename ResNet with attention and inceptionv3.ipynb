{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install torch torchvision"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"nW9dbf5lfko1","executionInfo":{"status":"ok","timestamp":1741182098687,"user_tz":-120,"elapsed":68670,"user":{"displayName":"מאור משה","userId":"08411296119985004086"}},"outputId":"809a244b-ebad-4a44-a093-28fa6ae25eb8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu124)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m92.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"]}]},{"cell_type":"code","source":["#mount my google drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","import pandas as pd\n","import os\n","from PIL import Image\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from PIL import Image\n","import torchvision.transforms as transforms\n","import torch.nn as nn\n","import torchvision.models as models\n","from sklearn.model_selection import train_test_split\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B_3jRhLCLPja","executionInfo":{"status":"ok","timestamp":1741182542938,"user_tz":-120,"elapsed":2395,"user":{"displayName":"מאור משה","userId":"08411296119985004086"}},"outputId":"4f0eeb2d-02c4-49d0-a833-34045d7b4bb7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["# 1. preper Dataset"],"metadata":{"id":"-KL5IPyubr5y"}},{"cell_type":"code","source":["train_df = pd.read_csv('/content/drive/MyDrive/Deep Final Work/Training dfs/w2v_train_df.csv')\n","#train_df = train_df.drop(columns=['Unnamed: 0'])\n","#train_df.head(5)\n","train_df = train_df.drop(columns=['ingredients', 'ingredient_embedding'])\n","train_df\n","\n","# Split train_df into training and evaluation sets (80/20 split)\n","train_df, eval_df = train_test_split(train_df, test_size=0.1, random_state=42)\n","print(\"Training images:\", len(train_df), \"Evaluation images:\", len(eval_df))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3OD8Rs-5ee4K","executionInfo":{"status":"ok","timestamp":1741182544023,"user_tz":-120,"elapsed":71,"user":{"displayName":"מאור משה","userId":"08411296119985004086"}},"outputId":"800011fe-53c2-4d5a-90e7-f3202bc228f9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training images: 2475 Evaluation images: 275\n"]}]},{"cell_type":"code","source":["# Fix the mistake in the 'path' column by replacing \"Depp\" with \"Deep\"\n","train_df['path'] = train_df['path'].str.replace(\"Depp\", \"Deep\")\n","eval_df['path']  = eval_df['path'].str.replace(\"Depp\", \"Deep\")\n"],"metadata":{"id":"Wg0ufqmoknUO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["MSE OR ASE ?"],"metadata":{"id":"aUr2usVFRW85"}},{"cell_type":"markdown","source":["create costum dataset"],"metadata":{"id":"qDK61tJoKLfL"}},{"cell_type":"code","source":["class FoodCaloriesDataset(Dataset):\n","    def __init__(self, df, transform=None):\n","        \"\"\"\n","        Args:\n","            df (pd.DataFrame): DataFrame with 'path' and 'calories' columns\n","            transform (callable, optional): Optional transform to be applied\n","                on a sample image.\n","        \"\"\"\n","        self.df = df.reset_index(drop=True)\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        row = self.df.iloc[idx]\n","        img_path = row['path']\n","        # Read the image\n","        image = Image.open(img_path).convert('RGB')\n","\n","        # Apply transforms if provided\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        # The label is the calorie count (float)\n","        label = torch.tensor(row['dish_calories'], dtype=torch.float)\n","\n","        return image, label\n","\n","\n","# Typical ImageNet means and stds\n","mean = [0.485, 0.456, 0.406]\n","std = [0.229, 0.224, 0.225]\n","\n","train_transform = transforms.Compose([\n","    transforms.Resize((299, 299)),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean, std),\n","])"],"metadata":{"id":"G0rBELphKLIV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the pretrained Inception V3\n","model = models.inception_v3(pretrained=True)\n","\n","# The original classifier head outputs 1000 classes for ImageNet\n","# We'll replace it with a single linear layer for regression\n","num_ftrs = model.fc.in_features\n","model.fc = nn.Linear(num_ftrs, 1)  # single output for calories\n","\n","# Inception V3 also has an auxiliary output by default.\n","# If you want to use it during training, you need to handle that.\n","# The simplest approach: disable the auxiliary by setting:\n","model.aux_logits = False\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ojKRrE_aKPUd","executionInfo":{"status":"ok","timestamp":1741184334711,"user_tz":-120,"elapsed":374,"user":{"displayName":"מאור משה","userId":"08411296119985004086"}},"outputId":"aba7f2ec-7894-4c6a-cc5c-9156d488e011"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]}]},{"cell_type":"code","source":["# make train_df['dish_calories'] a int\n","train_df['dish_calories'] = train_df['dish_calories'].astype(float)\n","eval_df['dish_calories'] = eval_df['dish_calories'].astype(float)\n","\n","\n","train_dataset = FoodCaloriesDataset(train_df, transform=train_transform)\n","val_dataset   = FoodCaloriesDataset(eval_df,   transform=train_transform)\n","\n","train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n","val_loader   = DataLoader(val_dataset,   batch_size=16, shuffle=False, num_workers=2)"],"metadata":{"id":"vHEpI7FfKiQU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["criterion = nn.MSELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n","\n","# If you have a GPU:\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"nIe05oVRKvx9","executionInfo":{"status":"ok","timestamp":1741184335041,"user_tz":-120,"elapsed":82,"user":{"displayName":"מאור משה","userId":"08411296119985004086"}},"outputId":"cb5298b3-94df-4246-80c8-c93018376573"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Inception3(\n","  (Conv2d_1a_3x3): BasicConv2d(\n","    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n","    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","  )\n","  (Conv2d_2a_3x3): BasicConv2d(\n","    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n","    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","  )\n","  (Conv2d_2b_3x3): BasicConv2d(\n","    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","  )\n","  (maxpool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (Conv2d_3b_1x1): BasicConv2d(\n","    (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","  )\n","  (Conv2d_4a_3x3): BasicConv2d(\n","    (conv): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)\n","    (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","  )\n","  (maxpool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (Mixed_5b): InceptionA(\n","    (branch1x1): BasicConv2d(\n","      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch5x5_1): BasicConv2d(\n","      (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch5x5_2): BasicConv2d(\n","      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n","      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch3x3dbl_1): BasicConv2d(\n","      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch3x3dbl_2): BasicConv2d(\n","      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch3x3dbl_3): BasicConv2d(\n","      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch_pool): BasicConv2d(\n","      (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (Mixed_5c): InceptionA(\n","    (branch1x1): BasicConv2d(\n","      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch5x5_1): BasicConv2d(\n","      (conv): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch5x5_2): BasicConv2d(\n","      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n","      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch3x3dbl_1): BasicConv2d(\n","      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch3x3dbl_2): BasicConv2d(\n","      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch3x3dbl_3): BasicConv2d(\n","      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch_pool): BasicConv2d(\n","      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (Mixed_5d): InceptionA(\n","    (branch1x1): BasicConv2d(\n","      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch5x5_1): BasicConv2d(\n","      (conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch5x5_2): BasicConv2d(\n","      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n","      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch3x3dbl_1): BasicConv2d(\n","      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch3x3dbl_2): BasicConv2d(\n","      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch3x3dbl_3): BasicConv2d(\n","      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch_pool): BasicConv2d(\n","      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (Mixed_6a): InceptionB(\n","    (branch3x3): BasicConv2d(\n","      (conv): Conv2d(288, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)\n","      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch3x3dbl_1): BasicConv2d(\n","      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch3x3dbl_2): BasicConv2d(\n","      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch3x3dbl_3): BasicConv2d(\n","      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), bias=False)\n","      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (Mixed_6b): InceptionC(\n","    (branch1x1): BasicConv2d(\n","      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7_1): BasicConv2d(\n","      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7_2): BasicConv2d(\n","      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n","      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7_3): BasicConv2d(\n","      (conv): Conv2d(128, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7dbl_1): BasicConv2d(\n","      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7dbl_2): BasicConv2d(\n","      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n","      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7dbl_3): BasicConv2d(\n","      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n","      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7dbl_4): BasicConv2d(\n","      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n","      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7dbl_5): BasicConv2d(\n","      (conv): Conv2d(128, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch_pool): BasicConv2d(\n","      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (Mixed_6c): InceptionC(\n","    (branch1x1): BasicConv2d(\n","      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7_1): BasicConv2d(\n","      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7_2): BasicConv2d(\n","      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n","      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7_3): BasicConv2d(\n","      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7dbl_1): BasicConv2d(\n","      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7dbl_2): BasicConv2d(\n","      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n","      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7dbl_3): BasicConv2d(\n","      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n","      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7dbl_4): BasicConv2d(\n","      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n","      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7dbl_5): BasicConv2d(\n","      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch_pool): BasicConv2d(\n","      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (Mixed_6d): InceptionC(\n","    (branch1x1): BasicConv2d(\n","      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7_1): BasicConv2d(\n","      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7_2): BasicConv2d(\n","      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n","      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7_3): BasicConv2d(\n","      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7dbl_1): BasicConv2d(\n","      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7dbl_2): BasicConv2d(\n","      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n","      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7dbl_3): BasicConv2d(\n","      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n","      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7dbl_4): BasicConv2d(\n","      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n","      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7dbl_5): BasicConv2d(\n","      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch_pool): BasicConv2d(\n","      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (Mixed_6e): InceptionC(\n","    (branch1x1): BasicConv2d(\n","      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7_1): BasicConv2d(\n","      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7_2): BasicConv2d(\n","      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7_3): BasicConv2d(\n","      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7dbl_1): BasicConv2d(\n","      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7dbl_2): BasicConv2d(\n","      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7dbl_3): BasicConv2d(\n","      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7dbl_4): BasicConv2d(\n","      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7dbl_5): BasicConv2d(\n","      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch_pool): BasicConv2d(\n","      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (AuxLogits): InceptionAux(\n","    (conv0): BasicConv2d(\n","      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (conv1): BasicConv2d(\n","      (conv): Conv2d(128, 768, kernel_size=(5, 5), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (fc): Linear(in_features=768, out_features=1000, bias=True)\n","  )\n","  (Mixed_7a): InceptionD(\n","    (branch3x3_1): BasicConv2d(\n","      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch3x3_2): BasicConv2d(\n","      (conv): Conv2d(192, 320, kernel_size=(3, 3), stride=(2, 2), bias=False)\n","      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7x3_1): BasicConv2d(\n","      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7x3_2): BasicConv2d(\n","      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7x3_3): BasicConv2d(\n","      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch7x7x3_4): BasicConv2d(\n","      (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (Mixed_7b): InceptionE(\n","    (branch1x1): BasicConv2d(\n","      (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch3x3_1): BasicConv2d(\n","      (conv): Conv2d(1280, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch3x3_2a): BasicConv2d(\n","      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n","      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch3x3_2b): BasicConv2d(\n","      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n","      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch3x3dbl_1): BasicConv2d(\n","      (conv): Conv2d(1280, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch3x3dbl_2): BasicConv2d(\n","      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch3x3dbl_3a): BasicConv2d(\n","      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n","      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch3x3dbl_3b): BasicConv2d(\n","      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n","      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch_pool): BasicConv2d(\n","      (conv): Conv2d(1280, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (Mixed_7c): InceptionE(\n","    (branch1x1): BasicConv2d(\n","      (conv): Conv2d(2048, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch3x3_1): BasicConv2d(\n","      (conv): Conv2d(2048, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch3x3_2a): BasicConv2d(\n","      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n","      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch3x3_2b): BasicConv2d(\n","      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n","      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch3x3dbl_1): BasicConv2d(\n","      (conv): Conv2d(2048, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch3x3dbl_2): BasicConv2d(\n","      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch3x3dbl_3a): BasicConv2d(\n","      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n","      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch3x3dbl_3b): BasicConv2d(\n","      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n","      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (branch_pool): BasicConv2d(\n","      (conv): Conv2d(2048, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (dropout): Dropout(p=0.5, inplace=False)\n","  (fc): Linear(in_features=2048, out_features=1, bias=True)\n",")"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":["str(train_df['path'][0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"3ErzvO51kcYl","executionInfo":{"status":"ok","timestamp":1741184335274,"user_tz":-120,"elapsed":9,"user":{"displayName":"מאור משה","userId":"08411296119985004086"}},"outputId":"a2110471-5c20-4823-e888-a79d78face1f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/Deep Final Work/Data/Nutrition5k_RGB/dish_1568318316_rgb.png'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":39}]},{"cell_type":"code","source":["# num_epochs = 10\n","\n","# for epoch in range(num_epochs):\n","#     model.train()\n","#     running_loss = 0.0\n","\n","#     for images, labels in train_loader:\n","#         images = images.to(device)\n","#         labels = labels.to(device).unsqueeze(1)  # shape (B,1)\n","\n","#         # Forward pass\n","#         outputs = model(images)\n","\n","#         # Compute loss\n","#         loss = criterion(outputs, labels)\n","\n","#         # Backprop\n","#         optimizer.zero_grad()\n","#         loss.backward()\n","#         optimizer.step()\n","\n","#         running_loss += loss.item() * images.size(0)\n","\n","#     epoch_loss = running_loss / len(train_loader.dataset)\n","\n","#     # Validation\n","#     model.eval()\n","#     val_loss = 0.0\n","#     with torch.no_grad():\n","#         for images, labels in val_loader:\n","#             images = images.to(device)\n","#             labels = labels.to(device).unsqueeze(1)\n","\n","#             outputs = model(images)\n","#             loss = criterion(outputs, labels)\n","#             val_loss += loss.item() * images.size(0)\n","\n","#     val_loss /= len(val_loader.dataset)\n","\n","#     print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n","#           f\"Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gb3sU8j0K7Os","outputId":"7bd7bf1b-87bd-4ae1-f82f-73cfac12601d","executionInfo":{"status":"ok","timestamp":1741184731413,"user_tz":-120,"elapsed":394581,"user":{"displayName":"מאור משה","userId":"08411296119985004086"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/10], Train Loss: 85422.2248, Val Loss: 136686.0862\n","Epoch [2/10], Train Loss: 76481.6838, Val Loss: 132542.1841\n","Epoch [3/10], Train Loss: 67857.1678, Val Loss: 114000.5904\n","Epoch [4/10], Train Loss: 57471.5943, Val Loss: 115451.9963\n","Epoch [5/10], Train Loss: 46693.1213, Val Loss: 98917.7820\n","Epoch [6/10], Train Loss: 38116.1578, Val Loss: 102308.6325\n","Epoch [7/10], Train Loss: 30812.7719, Val Loss: 91678.9394\n","Epoch [8/10], Train Loss: 25515.3554, Val Loss: 75397.3000\n","Epoch [9/10], Train Loss: 20665.2034, Val Loss: 76943.6385\n","Epoch [10/10], Train Loss: 17031.8403, Val Loss: 58045.1555\n"]}]},{"cell_type":"code","source":["# # Define the path where you want to save the model\n","# save_path = \"/content/drive/MyDrive/Deep Final Work/Saved Models/inception_v3_regression.pt\"\n","\n","# # Save the model state dictionary\n","# torch.save(model, save_path)\n","\n","# print(f\"Model saved to {save_path}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ElXlnTn8Wpzd","executionInfo":{"status":"ok","timestamp":1741184731601,"user_tz":-120,"elapsed":185,"user":{"displayName":"מאור משה","userId":"08411296119985004086"}},"outputId":"9abe68b1-99fb-4ee2-e80b-0f719479859c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model saved to /content/drive/MyDrive/Deep Final Work/Saved Models/inception_v3_regression.pt\n"]}]},{"cell_type":"code","source":["# model.eval()\n","# test_img_path = \"/content/drive/MyDrive/Deep Final Work/Nutrition5k_RGB/dish_1562686492_rgb.png\"\n","# for i in x[:10]:\n","#   test_img_path = i\n","#   with torch.no_grad():\n","#       img = Image.open(test_img_path).convert('RGB')\n","#       img = train_transform(img).unsqueeze(0).to(device)  # same transforms as training\n","#       pred_calories = model(img)\n","#       print(\"Predicted calories:\", pred_calories.item())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"Rk5jrztDU9fT","executionInfo":{"status":"error","timestamp":1741184731635,"user_tz":-120,"elapsed":21,"user":{"displayName":"מאור משה","userId":"08411296119985004086"}},"outputId":"1dbbcb7e-40c8-4ec7-9849-3e6b55f7828b"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'x' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-42-4e1e17845632>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtest_img_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/Deep Final Work/Nutrition5k_RGB/dish_1562686492_rgb.png\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mtest_img_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"]}]},{"cell_type":"code","source":["eval_df.head(5)"],"metadata":{"id":"i1LFRvlrVcEX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"aZS6X6ReKBBN"}},{"cell_type":"markdown","source":["          +---------------------+\n","          |  Inception (body)  |\n","          |   (pretrained)     |\n","          +---------+----------+\n","                    |\n","                Feature Map\n","                    |\n","               Flatten / GAP\n","                    |\n","       +-------------+-------------+\n","       |  FC1 -> ReLU -> Dropout  |\n","       +-------------+-------------+\n","                    |\n","       +-------------+-------------+\n","       |  FC2 -> ReLU -> Dropout  |\n","       +-------------+-------------+\n","                    |\n","       +-------------+-------------+\n","       |        FC3 -> Output     |\n","       +-------------+-------------+\n","                    |\n","               Predicted Calories\n"],"metadata":{"id":"BpvG9ugEKA4O"}},{"cell_type":"code","source":["!pip install torch torchvision  # Uncomment if you need to install\n"],"metadata":{"id":"UPghnTLAdc2K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","import torch\n","import torch.nn as nn\n","import torchvision.transforms as transforms\n","import torchvision.models as models\n","from torch.utils.data import Dataset, DataLoader\n","import pandas as pd\n","from PIL import Image\n","import os\n","\n","#####################################\n","# 1. Custom Dataset\n","#####################################\n","class FoodCaloriesDataset(Dataset):\n","    def __init__(self, df, transform=None):\n","        \"\"\"\n","        Args:\n","            df (pd.DataFrame): Must contain 'path' and 'calories' columns\n","            transform (callable, optional): Optional transform to be applied\n","                on an image.\n","        \"\"\"\n","        self.df = df.reset_index(drop=True)\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        row = self.df.iloc[idx]\n","        img_path = row['path']\n","        # Read the image\n","        image = Image.open(img_path).convert('RGB')\n","\n","        # Apply transforms if provided\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        # The label is the calorie count (float)\n","        label = torch.tensor(row['dish_calories'], dtype=torch.float)\n","\n","        return image, label\n","\n","\n","#####################################\n","# 2. Transforms\n","#####################################\n","# Typical ImageNet means and stds\n","mean = [0.485, 0.456, 0.406]\n","std = [0.229, 0.224, 0.225]\n","\n","train_transform = transforms.Compose([\n","    transforms.Resize((299, 299)),      # for Inception V3\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean, std),\n","])\n","\n","\n","#####################################\n","# 3. Enhanced Inception Model\n","#####################################\n","class InceptionCalorieRegressor(nn.Module):\n","    \"\"\"\n","    An InceptionV3-based model that:\n","      - Takes an image input\n","      - Extracts features with the pretrained InceptionV3 body\n","      - Passes those features through multiple FC layers\n","      - Outputs a single numeric value (calories)\n","    \"\"\"\n","    def __init__(self, pretrained=True):\n","        super(InceptionCalorieRegressor, self).__init__()\n","\n","        # Load the Inception V3 model\n","        inception = models.inception_v3(pretrained=pretrained)\n","\n","        # Disable the auxiliary classifier for simplicity\n","        inception.aux_logits = False\n","\n","        # Extract the number of input features of the final FC layer\n","        in_features = inception.fc.in_features\n","\n","        # Replace the final FC layer with an Identity layer\n","        # so we can add our own multi-layer head\n","        inception.fc = nn.Identity()\n","\n","        self.inception_body = inception\n","\n","        # multi-layer FC head\n","        self.fc_head = nn.Sequential(\n","            nn.Linear(in_features, 512),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(p=0.3),\n","            nn.Linear(512, 128),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(p=0.3),\n","            nn.Linear(128, 1)  # Single output for calorie regression\n","        )\n","\n","    def forward(self, x):\n","        # Pass the input through Inception (up to the final pool)\n","        features = self.inception_body(x)  # shape: (batch_size, in_features)\n","\n","        # Then pass the extracted features through your FC layers\n","        out = self.fc_head(features)\n","        return out\n","\n","\n","#####################################\n","# 4. Datasets and DataLoaders\n","#####################################\n","\n","\n","# Ensure dish_calories are float\n","train_df['dish_calories'] = train_df['dish_calories'].astype(float)\n","eval_df['dish_calories']  = eval_df['dish_calories'].astype(float)\n","\n","train_dataset = FoodCaloriesDataset(train_df, transform=train_transform)\n","val_dataset   = FoodCaloriesDataset(eval_df,   transform=train_transform)\n","\n","train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True,  num_workers=2)\n","val_loader   = DataLoader(val_dataset,   batch_size=16, shuffle=False, num_workers=2)\n","\n","\n","#####################################\n","# 5. Initialize Model, Loss, Optimizer\n","#####################################\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","model = InceptionCalorieRegressor(pretrained=True)\n","model.to(device)\n","\n","criterion = nn.MSELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n","\n","\n","#####################################\n","# 6. Training Loop\n","#####################################\n","num_epochs = 15\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    running_loss = 0.0\n","\n","    for images, labels in train_loader:\n","        images = images.to(device)\n","        # shape (batch_size,) => make it (batch_size,1) for regression\n","        labels = labels.to(device).unsqueeze(1)\n","\n","        # Forward pass\n","        outputs = model(images)  # shape (batch_size, 1)\n","\n","        # Compute loss\n","        loss = criterion(outputs, labels)\n","\n","        # Backprop\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item() * images.size(0)\n","\n","    epoch_loss = running_loss / len(train_loader.dataset)\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0.0\n","    with torch.no_grad():\n","        for images, labels in val_loader:\n","            images = images.to(device)\n","            labels = labels.to(device).unsqueeze(1)\n","\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            val_loss += loss.item() * images.size(0)\n","\n","    val_loss /= len(val_loader.dataset)\n","\n","    print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n","          f\"Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}\")\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"T8yAl7zbcgD0","executionInfo":{"status":"ok","timestamp":1741187049503,"user_tz":-120,"elapsed":409545,"user":{"displayName":"מאור משה","userId":"08411296119985004086"}},"outputId":"4b122606-1ab0-4518-dd8c-8be2946f720a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/15], Train Loss: 54812.2558, Val Loss: 62752.2488\n","Epoch [2/15], Train Loss: 12687.3069, Val Loss: 61820.0371\n","Epoch [3/15], Train Loss: 10247.5480, Val Loss: 59623.7319\n","Epoch [4/15], Train Loss: 7033.2040, Val Loss: 58756.3218\n","Epoch [5/15], Train Loss: 7378.1842, Val Loss: 62287.4041\n","Epoch [6/15], Train Loss: 6137.7094, Val Loss: 59783.3956\n","Epoch [7/15], Train Loss: 5272.4885, Val Loss: 61877.2815\n","Epoch [8/15], Train Loss: 4426.9605, Val Loss: 61628.4608\n","Epoch [9/15], Train Loss: 4492.0447, Val Loss: 60923.6448\n","Epoch [10/15], Train Loss: 3750.7159, Val Loss: 60097.6902\n","Epoch [11/15], Train Loss: 3705.3992, Val Loss: 62408.4389\n","Epoch [12/15], Train Loss: 3137.2733, Val Loss: 58914.4918\n","Epoch [13/15], Train Loss: 2981.3810, Val Loss: 61287.0541\n","Epoch [14/15], Train Loss: 3152.3913, Val Loss: 58892.8155\n","Epoch [15/15], Train Loss: 2765.4417, Val Loss: 62776.7671\n"]}]},{"cell_type":"code","source":["# Define the path where you want to save the ResNet model\n","save_path = \"/content/drive/MyDrive/Deep Final Work/Saved Models/GOOGLES_MODEL_RESORATION.pt\"\n","\n","# Save the model state dictionary\n","torch.save(model, save_path)\n","\n","print(f\"Model saved to {save_path}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VuH2qm2PX6ZU","executionInfo":{"status":"ok","timestamp":1741187049830,"user_tz":-120,"elapsed":324,"user":{"displayName":"מאור משה","userId":"08411296119985004086"}},"outputId":"fd308f79-265f-42f1-dc01-ff88410bbec7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model saved to /content/drive/MyDrive/Deep Final Work/Saved Models/GOOGLES_MODEL_RESORATION.pt\n"]}]},{"cell_type":"code","source":["#####################################\n","# 7. Inference Example\n","#####################################\n","model.eval()\n","eval_paths = eval_df['path'].tolist()\n","for path in eval_paths[:10]:\n","  test_img_path = path\n","  with torch.no_grad():\n","      img = Image.open(test_img_path).convert('RGB')\n","      img = train_transform(img).unsqueeze(0).to(device)  # same transforms as training\n","      pred_calories = model(img)\n","      print(\"Predicted calories:\", pred_calories.item() ,\n","            \"real caloric value:\", eval_df['dish_calories'][eval_df['path']==path])\n"],"metadata":{"id":"sFpgfM68iJro"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ResNet with CBAM"],"metadata":{"id":"-es7kYpGnYxu"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torchvision.transforms as transforms\n","import torchvision.models as models\n","from torch.utils.data import Dataset, DataLoader\n","import pandas as pd\n","from PIL import Image\n","import os\n","\n","#####################################\n","# 1. Custom Dataset\n","#####################################\n","class FoodCaloriesDataset(Dataset):\n","    def __init__(self, df, transform=None):\n","        \"\"\"\n","        Args:\n","            df (pd.DataFrame): Must contain 'path' and 'dish_calories' columns.\n","            transform (callable, optional): Optional transform to be applied on an image.\n","        \"\"\"\n","        self.df = df.reset_index(drop=True)\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        row = self.df.iloc[idx]\n","        img_path = row['path']\n","        image = Image.open(img_path).convert('RGB')\n","        if self.transform:\n","            image = self.transform(image)\n","        # Convert dish_calories to a float tensor.\n","        label = torch.tensor(row['dish_calories'], dtype=torch.float)\n","        return image, label\n","\n","#####################################\n","# 2. Transforms\n","#####################################\n","# Standard ImageNet normalization parameters.\n","mean = [0.485, 0.456, 0.406]\n","std = [0.229, 0.224, 0.225]\n","\n","# Resize images to 224x224, the typical size for ResNet.\n","train_transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean, std),\n","])\n","\n","#####################################\n","# 3. CBAM (Convolutional Block Attention Module)\n","#####################################\n","class ChannelAttention(nn.Module):\n","    def __init__(self, in_channels, reduction=16):\n","        super(ChannelAttention, self).__init__()\n","        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n","        self.max_pool = nn.AdaptiveMaxPool2d(1)\n","        self.fc = nn.Sequential(\n","            nn.Conv2d(in_channels, in_channels // reduction, kernel_size=1, bias=False),\n","            nn.ReLU(),\n","            nn.Conv2d(in_channels // reduction, in_channels, kernel_size=1, bias=False)\n","        )\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        avg_out = self.fc(self.avg_pool(x))\n","        max_out = self.fc(self.max_pool(x))\n","        out = avg_out + max_out\n","        return self.sigmoid(out)\n","\n","class SpatialAttention(nn.Module):\n","    def __init__(self, kernel_size=7):\n","        super(SpatialAttention, self).__init__()\n","        padding = kernel_size // 2\n","        self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=padding, bias=False)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        avg_out = torch.mean(x, dim=1, keepdim=True)\n","        max_out, _ = torch.max(x, dim=1, keepdim=True)\n","        x_cat = torch.cat([avg_out, max_out], dim=1)\n","        out = self.conv(x_cat)\n","        return self.sigmoid(out)\n","\n","class CBAM(nn.Module):\n","    def __init__(self, in_channels, reduction=16, kernel_size=7):\n","        super(CBAM, self).__init__()\n","        self.channel_attention = ChannelAttention(in_channels, reduction)\n","        self.spatial_attention = SpatialAttention(kernel_size)\n","\n","    def forward(self, x):\n","        out = x * self.channel_attention(x)\n","        out = out * self.spatial_attention(out)\n","        return out\n","\n","#####################################\n","# 4. ResNet with CBAM for Calorie Regression\n","#####################################\n","class ResNetAttentionCalorieRegressor(nn.Module):\n","    \"\"\"\n","    A ResNet50-based network enhanced with CBAM attention layers.\n","    The network:\n","      - Uses a pretrained ResNet50 backbone (up to the last conv layer).\n","      - Applies CBAM attention to refine the extracted feature maps.\n","      - Pools the features and passes them through a custom fully connected head.\n","      - Outputs a single value for calorie regression.\n","    \"\"\"\n","    def __init__(self, pretrained=True):\n","        super(ResNetAttentionCalorieRegressor, self).__init__()\n","        # Load a pretrained ResNet50 and remove the average pooling and fc layers.\n","        resnet = models.resnet50(pretrained=pretrained)\n","        self.backbone = nn.Sequential(*list(resnet.children())[:-2])  # output shape: (B, 2048, H, W)\n","\n","        # Add CBAM attention after the backbone.\n","        self.cbam = CBAM(in_channels=2048, reduction=16, kernel_size=7)\n","\n","        # Global average pooling to convert (B, 2048, H, W) to (B, 2048, 1, 1)\n","        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n","\n","        # Custom fully connected head.\n","        self.fc_head = nn.Sequential(\n","            nn.Linear(2048, 512),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(0.3),\n","            nn.Linear(512, 128),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(0.3),\n","            nn.Linear(128, 1)  # Single output for regression.\n","        )\n","\n","    def forward(self, x):\n","        x = self.backbone(x)      # (B, 2048, H, W)\n","        x = self.cbam(x)          # Refined with attention.\n","        x = self.avgpool(x)       # (B, 2048, 1, 1)\n","        x = torch.flatten(x, 1)   # (B, 2048)\n","        out = self.fc_head(x)     # (B, 1)\n","        return out\n","\n","#####################################\n","# 5. Datasets and DataLoaders\n","#####################################\n","train_df['dish_calories'] = train_df['dish_calories'].astype(float)\n","eval_df['dish_calories']  = eval_df['dish_calories'].astype(float)\n","\n","train_dataset = FoodCaloriesDataset(train_df, transform=train_transform)\n","val_dataset   = FoodCaloriesDataset(eval_df, transform=train_transform)\n","\n","train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n","val_loader   = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2)\n","\n","#####################################\n","# 6. Initialize Model, Loss, Optimizer\n","#####################################\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = ResNetAttentionCalorieRegressor(pretrained=True)\n","model.to(device)\n","\n","criterion = nn.MSELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n","\n","#####################################\n","# 7. Training Loop\n","#####################################\n","num_epochs = 10\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    running_loss = 0.0\n","\n","    for images, labels in train_loader:\n","        images = images.to(device)\n","        labels = labels.to(device).unsqueeze(1)  # Reshape (B,) -> (B, 1)\n","\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item() * images.size(0)\n","\n","    epoch_loss = running_loss / len(train_loader.dataset)\n","\n","    # Validation phase.\n","    model.eval()\n","    val_loss = 0.0\n","    with torch.no_grad():\n","        for images, labels in val_loader:\n","            images = images.to(device)\n","            labels = labels.to(device).unsqueeze(1)\n","\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            val_loss += loss.item() * images.size(0)\n","\n","    val_loss /= len(val_loader.dataset)\n","\n","    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}\")\n","\n","#####################################\n","# 8. Saving the Model (Optional)\n","#####################################\n","save_path = \"/content/drive/MyDrive/Deep Final Work/Saved Models/resnet_attention_calorie_regressor.pth\"\n","torch.save(model, save_path)\n","print(\"Model saved successfully!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qPe4e2pGfriJ","executionInfo":{"status":"ok","timestamp":1741186639947,"user_tz":-120,"elapsed":256094,"user":{"displayName":"מאור משה","userId":"08411296119985004086"}},"outputId":"eabbe8c0-e7d2-49cf-cb7b-33f39b1b4e8c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/10], Train Loss: 49931.8699, Val Loss: 78778.7679\n","Epoch [2/10], Train Loss: 13324.9813, Val Loss: 61487.1021\n","Epoch [3/10], Train Loss: 10397.7124, Val Loss: 60759.5048\n","Epoch [4/10], Train Loss: 8876.3664, Val Loss: 58624.1554\n","Epoch [5/10], Train Loss: 7646.0468, Val Loss: 60968.0321\n","Epoch [6/10], Train Loss: 5833.0889, Val Loss: 59260.2897\n","Epoch [7/10], Train Loss: 6288.3176, Val Loss: 58662.4073\n","Epoch [8/10], Train Loss: 4844.1687, Val Loss: 58434.0910\n","Epoch [9/10], Train Loss: 4640.6567, Val Loss: 56882.9093\n","Epoch [10/10], Train Loss: 4265.1080, Val Loss: 57310.7985\n","Model saved successfully!\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"LBWoBU1ppgj5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torchvision.transforms as transforms\n","import torchvision.models as models\n","from torch.utils.data import Dataset, DataLoader\n","import pandas as pd\n","from PIL import Image\n","import os\n","\n","#####################################\n","# 1. Custom Dataset\n","#####################################\n","class FoodCaloriesDataset(Dataset):\n","    def __init__(self, df, transform=None):\n","        \"\"\"\n","        Args:\n","            df (pd.DataFrame): Must contain 'path' and 'dish_calories' columns.\n","            transform (callable, optional): Optional transform to be applied on an image.\n","        \"\"\"\n","        self.df = df.reset_index(drop=True)\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        row = self.df.iloc[idx]\n","        img_path = row['path']\n","        image = Image.open(img_path).convert('RGB')\n","        if self.transform:\n","            image = self.transform(image)\n","        # Convert dish_calories to a float tensor.\n","        label = torch.tensor(row['dish_calories'], dtype=torch.float)\n","        return image, label\n","\n","#####################################\n","# 2. Transforms\n","#####################################\n","mean = [0.485, 0.456, 0.406]\n","std = [0.229, 0.224, 0.225]\n","\n","train_transform = transforms.Compose([\n","    transforms.Resize((224, 224)),  # Standard for ResNet\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean, std),\n","])\n","\n","#####################################\n","# 3. CBAM (Convolutional Block Attention Module)\n","#####################################\n","class ChannelAttention(nn.Module):\n","    def __init__(self, in_channels, reduction=16):\n","        super(ChannelAttention, self).__init__()\n","        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n","        self.max_pool = nn.AdaptiveMaxPool2d(1)\n","        self.fc = nn.Sequential(\n","            nn.Conv2d(in_channels, in_channels // reduction, kernel_size=1, bias=False),\n","            nn.ReLU(),\n","            nn.Conv2d(in_channels // reduction, in_channels, kernel_size=1, bias=False)\n","        )\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        avg_out = self.fc(self.avg_pool(x))\n","        max_out = self.fc(self.max_pool(x))\n","        out = avg_out + max_out\n","        return self.sigmoid(out)\n","\n","class SpatialAttention(nn.Module):\n","    def __init__(self, kernel_size=7):\n","        super(SpatialAttention, self).__init__()\n","        padding = kernel_size // 2\n","        self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=padding, bias=False)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        avg_out = torch.mean(x, dim=1, keepdim=True)\n","        max_out, _ = torch.max(x, dim=1, keepdim=True)\n","        x_cat = torch.cat([avg_out, max_out], dim=1)\n","        out = self.conv(x_cat)\n","        return self.sigmoid(out)\n","\n","class CBAM(nn.Module):\n","    def __init__(self, in_channels, reduction=16, kernel_size=7):\n","        super(CBAM, self).__init__()\n","        self.channel_attention = ChannelAttention(in_channels, reduction)\n","        self.spatial_attention = SpatialAttention(kernel_size)\n","\n","    def forward(self, x):\n","        out = x * self.channel_attention(x)\n","        out = out * self.spatial_attention(out)\n","        return out\n","\n","#####################################\n","# 4. ResNet with CBAM for Calorie Regression\n","#####################################\n","class ResNetAttentionCalorieRegressor(nn.Module):\n","    def __init__(self, pretrained=True):\n","        super(ResNetAttentionCalorieRegressor, self).__init__()\n","        # Load pretrained ResNet50, excluding the final pooling and FC layers.\n","        resnet = models.resnet50(pretrained=pretrained)\n","        self.backbone = nn.Sequential(*list(resnet.children())[:-2])  # (B, 2048, H, W)\n","\n","        # Add CBAM attention module.\n","        self.cbam = CBAM(in_channels=2048, reduction=16, kernel_size=7)\n","\n","        # Global average pooling to get (B, 2048, 1, 1)\n","        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n","\n","        # Custom fully connected head.\n","        self.fc_head = nn.Sequential(\n","            nn.Linear(2048, 512),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(0.3),\n","            nn.Linear(512, 128),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(0.3),\n","            nn.Linear(128, 1)  # Single output for regression.\n","        )\n","\n","    def forward(self, x):\n","        x = self.backbone(x)      # (B, 2048, H, W)\n","        x = self.cbam(x)          # Apply attention.\n","        x = self.avgpool(x)       # (B, 2048, 1, 1)\n","        x = torch.flatten(x, 1)   # (B, 2048)\n","        out = self.fc_head(x)     # (B, 1)\n","        return out\n","\n","#####################################\n","# 5. Datasets and DataLoaders\n","#####################################\n","# Ensure your train_df and eval_df have 'path' and 'dish_calories' columns.\n","train_df['dish_calories'] = train_df['dish_calories'].astype(float)\n","eval_df['dish_calories']  = eval_df['dish_calories'].astype(float)\n","\n","train_dataset = FoodCaloriesDataset(train_df, transform=train_transform)\n","# Even if you don't use eval during training, you can still define an eval dataset if needed later.\n","eval_dataset   = FoodCaloriesDataset(eval_df, transform=train_transform)\n","\n","train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n","\n","#####################################\n","# 6. Initialize Model, Loss, Optimizer\n","#####################################\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = ResNetAttentionCalorieRegressor(pretrained=True)\n","model.to(device)\n","\n","criterion = nn.MSELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n","\n","#####################################\n","# 7. Training Loop (No Eval Iteration)\n","#####################################\n","num_epochs = 10\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    running_loss = 0.0\n","\n","    for images, labels in train_loader:\n","        images = images.to(device)\n","        labels = labels.to(device).unsqueeze(1)  # Reshape to (B, 1)\n","\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item() * images.size(0)\n","\n","    epoch_loss = running_loss / len(train_loader.dataset)\n","    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}\")\n","\n","#####################################\n","# 8. Saving the Model (Optional)\n","#####################################\n","save_path = \"/content/drive/MyDrive/Deep Final Work/Saved Models/resnet_attention_calorie_regressor.pth\"\n","torch.save(model, save_path)\n","print(\"Model saved successfully!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iiDppdIsnbAQ","executionInfo":{"status":"ok","timestamp":1741186383822,"user_tz":-120,"elapsed":241909,"user":{"displayName":"מאור משה","userId":"08411296119985004086"}},"outputId":"8eff46b3-9c82-4d64-d3e7-75872ca84d41"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/10], Train Loss: 62826.5916\n","Epoch [2/10], Train Loss: 13298.2276\n","Epoch [3/10], Train Loss: 11553.0823\n","Epoch [4/10], Train Loss: 9261.1181\n","Epoch [5/10], Train Loss: 8149.5509\n","Epoch [6/10], Train Loss: 5866.4625\n","Epoch [7/10], Train Loss: 6933.7288\n","Epoch [8/10], Train Loss: 5706.4182\n","Epoch [9/10], Train Loss: 5107.4281\n","Epoch [10/10], Train Loss: 4344.7108\n","Model saved successfully!\n"]}]},{"cell_type":"code","source":["!pip install torchviz\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c2BzuLiIpeO2","executionInfo":{"status":"ok","timestamp":1741100658608,"user_tz":-120,"elapsed":2360,"user":{"displayName":"מאור משה","userId":"08411296119985004086"}},"outputId":"f2cca801-b38e-4a7b-be78-f260f25dad38"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torchviz\n","  Downloading torchviz-0.0.3-py3-none-any.whl.metadata (2.1 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from torchviz) (2.5.1+cu124)\n","Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from torchviz) (0.20.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (3.17.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (3.1.5)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (2024.10.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (12.3.1.170)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (12.4.127)\n","Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (3.1.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->torchviz) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->torchviz) (3.0.2)\n","Downloading torchviz-0.0.3-py3-none-any.whl (5.7 kB)\n","Installing collected packages: torchviz\n","Successfully installed torchviz-0.0.3\n"]}]},{"cell_type":"code","source":["from torchviz import make_dot\n","\n","# Create a dummy input with the shape expected by your model (batch_size=1, channels=3, height=224, width=224)\n","dummy_input = torch.randn(1, 3, 224, 224).to(device)\n","\n","# Get the model's output for the dummy input\n","output = model(dummy_input)\n","\n","# Generate the computational graph\n","dot = make_dot(output, params=dict(model.named_parameters()))\n","dot.format = \"png\"  # you can change the format as needed\n","dot.render(\"resnet_attention_calorie_regressor_graph\")\n","\n","print(\"Model visualization saved as 'resnet_attention_calorie_regressor_graph.png'\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fz5p31gjr6WH","executionInfo":{"status":"ok","timestamp":1741100664707,"user_tz":-120,"elapsed":2126,"user":{"displayName":"מאור משה","userId":"08411296119985004086"}},"outputId":"d23b93f5-de18-42a2-bedd-287a9a853fb9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model visualization saved as 'resnet_attention_calorie_regressor_graph.png'\n"]}]},{"cell_type":"code","source":["!pip install torchinfo\n","\n","from torchinfo import summary\n","\n","model = ResNetAttentionCalorieRegressor(pretrained=True).cuda()\n","summary(model, input_size=(16, 3, 224, 224))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dj16P-hFr75H","executionInfo":{"status":"ok","timestamp":1741100816522,"user_tz":-120,"elapsed":2862,"user":{"displayName":"מאור משה","userId":"08411296119985004086"}},"outputId":"38879120-692c-4135-ab8c-662fae25f7a8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torchinfo\n","  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n","Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n","Installing collected packages: torchinfo\n","Successfully installed torchinfo-1.8.0\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"execute_result","data":{"text/plain":["===============================================================================================\n","Layer (type:depth-idx)                        Output Shape              Param #\n","===============================================================================================\n","ResNetAttentionCalorieRegressor               [16, 1]                   --\n","├─Sequential: 1-1                             [16, 2048, 7, 7]          --\n","│    └─Conv2d: 2-1                            [16, 64, 112, 112]        9,408\n","│    └─BatchNorm2d: 2-2                       [16, 64, 112, 112]        128\n","│    └─ReLU: 2-3                              [16, 64, 112, 112]        --\n","│    └─MaxPool2d: 2-4                         [16, 64, 56, 56]          --\n","│    └─Sequential: 2-5                        [16, 256, 56, 56]         --\n","│    │    └─Bottleneck: 3-1                   [16, 256, 56, 56]         75,008\n","│    │    └─Bottleneck: 3-2                   [16, 256, 56, 56]         70,400\n","│    │    └─Bottleneck: 3-3                   [16, 256, 56, 56]         70,400\n","│    └─Sequential: 2-6                        [16, 512, 28, 28]         --\n","│    │    └─Bottleneck: 3-4                   [16, 512, 28, 28]         379,392\n","│    │    └─Bottleneck: 3-5                   [16, 512, 28, 28]         280,064\n","│    │    └─Bottleneck: 3-6                   [16, 512, 28, 28]         280,064\n","│    │    └─Bottleneck: 3-7                   [16, 512, 28, 28]         280,064\n","│    └─Sequential: 2-7                        [16, 1024, 14, 14]        --\n","│    │    └─Bottleneck: 3-8                   [16, 1024, 14, 14]        1,512,448\n","│    │    └─Bottleneck: 3-9                   [16, 1024, 14, 14]        1,117,184\n","│    │    └─Bottleneck: 3-10                  [16, 1024, 14, 14]        1,117,184\n","│    │    └─Bottleneck: 3-11                  [16, 1024, 14, 14]        1,117,184\n","│    │    └─Bottleneck: 3-12                  [16, 1024, 14, 14]        1,117,184\n","│    │    └─Bottleneck: 3-13                  [16, 1024, 14, 14]        1,117,184\n","│    └─Sequential: 2-8                        [16, 2048, 7, 7]          --\n","│    │    └─Bottleneck: 3-14                  [16, 2048, 7, 7]          6,039,552\n","│    │    └─Bottleneck: 3-15                  [16, 2048, 7, 7]          4,462,592\n","│    │    └─Bottleneck: 3-16                  [16, 2048, 7, 7]          4,462,592\n","├─CBAM: 1-2                                   [16, 2048, 7, 7]          --\n","│    └─ChannelAttention: 2-9                  [16, 2048, 1, 1]          --\n","│    │    └─AdaptiveAvgPool2d: 3-17           [16, 2048, 1, 1]          --\n","│    │    └─Sequential: 3-18                  [16, 2048, 1, 1]          524,288\n","│    │    └─AdaptiveMaxPool2d: 3-19           [16, 2048, 1, 1]          --\n","│    │    └─Sequential: 3-20                  [16, 2048, 1, 1]          (recursive)\n","│    │    └─Sigmoid: 3-21                     [16, 2048, 1, 1]          --\n","│    └─SpatialAttention: 2-10                 [16, 1, 7, 7]             --\n","│    │    └─Conv2d: 3-22                      [16, 1, 7, 7]             98\n","│    │    └─Sigmoid: 3-23                     [16, 1, 7, 7]             --\n","├─AdaptiveAvgPool2d: 1-3                      [16, 2048, 1, 1]          --\n","├─Sequential: 1-4                             [16, 1]                   --\n","│    └─Linear: 2-11                           [16, 512]                 1,049,088\n","│    └─ReLU: 2-12                             [16, 512]                 --\n","│    └─Dropout: 2-13                          [16, 512]                 --\n","│    └─Linear: 2-14                           [16, 128]                 65,664\n","│    └─ReLU: 2-15                             [16, 128]                 --\n","│    └─Dropout: 2-16                          [16, 128]                 --\n","│    └─Linear: 2-17                           [16, 1]                   129\n","===============================================================================================\n","Total params: 25,147,299\n","Trainable params: 25,147,299\n","Non-trainable params: 0\n","Total mult-adds (Units.GIGABYTES): 65.43\n","===============================================================================================\n","Input size (MB): 9.63\n","Forward/backward pass size (MB): 2845.83\n","Params size (MB): 100.59\n","Estimated Total Size (MB): 2956.05\n","==============================================================================================="]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":[],"metadata":{"id":"gL7F9avKuYrv"},"execution_count":null,"outputs":[]}]}